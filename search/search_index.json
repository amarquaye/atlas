{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>ATLAS is a final year project by Jesse Amarquaye and Greatman Akomea, computer engineering students from Ghana Communication Technology University.</p>"},{"location":"#introduction","title":"Introduction","text":"<p>Atlas is a hallucination detector for Large Language Models. Its main focus is on generative text as that is the most widely used medium for interacting with LLMs.</p>"},{"location":"#why-work-on-hallucination-in-llms","title":"Why work on hallucination in LLMs?","text":"<p>Question</p> <p>Large language models (LLMs) are revolutionizing human-computer interaction, generating increasingly fluent and human-like text. However, a significant challenge in LLMs is their tendency to produce hallucinations, or factually incorrect, nonsensical, or misleading content. As humans become increasingly reliant on LLMs for information and decision-making, ensuring their reliability and accuracy is crucial. This project aims to address this challenge by developing a software for detecting and mitigating hallucinations in LLMs so users can rely on LLM outputs with greater confidence, leading to wider adoption and societal benefits and also reduces the risk of misinformation to promote responsible use of LLMs.</p>"},{"location":"#why-atlas","title":"Why Atlas?","text":"<p>Question</p> <p>The story of Atlas in Greek mythology is closely tied to his role in supporting the heavens. According to one myth, during the Titanomachy; the war between the Titans and the Olympian gods, Atlas sided with the Titans. After their defeat, Zeus condemned Atlas to bear the weight of the heavens on his shoulders for eternity.</p> <p>The name Atlas symbolizes the software's commitment to bearing the responsibility of overseeing the cognitive aspects of language models, maintaining their stability, and preventing them from collapsing into inaccuracies or hallucinations. The association with Atlas also conveys strength, resilience, and reliability, suggesting a software that can handle the weight of complex language processing tasks with steadfastness and precision.</p>"},{"location":"#aims-or-objectives","title":"Aims or Objectives","text":"<p>Success</p> <ul> <li>Explore techniques for mitigating hallucinations in LLMs.</li> <li>Develop a software for automatic detection of hallucinations in LLMs.</li> <li>Evaluate the effectiveness of the developed tool in different LLMs.</li> </ul>"},{"location":"#methodology","title":"Methodology","text":"<p>Our approach will be the design and implementation of a software to detect or flag and mitigate hallucinations in LLMs. The ultimate objective is the creation of a browser extension or website to actively scan the output from LLMs and compare them with results from trusted sources on the web and inform the user of any occurrence of hallucinations.</p> View Flow Diagram <p></p>"},{"location":"#what-we-plan-to-do","title":"What we plan to do","text":"<p>Success</p> <ul> <li>Detect and mitigate hallucinations in the form of generated text.</li> </ul>"},{"location":"#what-we-do-not-plan-to-do","title":"What we do not plan to do","text":"<p>Tip</p> <ul> <li>Detecting and mitigating hallucination in images generated by LLMs.</li> <li>Detecting and mitigating hallucination in videos generated by LLMs.</li> </ul>"},{"location":"#plan","title":"Plan","text":"<p>Success</p> <ul> <li>Develop an API to search the web. Click to try our interative documentation.</li> <li>Scrape the contents of the resulting links from the search.</li> <li>Summarize the contents of the extracted text.</li> <li>Identify the difference between the LLM's response and the results from our search.</li> <li>Prompt the user of potential hallucinations if any are detected.</li> <li>Mitigate by allowing the users to delve deeper and if they are satisfied, substitute the LLM's response with ours.</li> <li>Create a site to test how atlas will detect hallucinations in LLMs.</li> <li>Create browser extension to finally test how atlas will operate.</li> </ul>"},{"location":"#results-and-analysis","title":"Results And Analysis","text":"<p>During our tests or evaluations we use two of the most popular LLMs, OpenAI's ChatGPT and Google's Gemini.</p> <p>Info</p> <ul> <li>ChatGPT returned 120 accurate responses out of 200.</li> <li> <p>Gemini returned 98 accurate responses out of 200.</p> </li> <li> <p>Both LLMs returned 82 responses that were hallucinations(even though they happened for different questions).</p> </li> </ul> <p>Note</p> <p>We noticed that ChatGPT was more accurate than Gemini in most cases and Gemini was more prone to hallucinations.</p> <p>ChatGPT seems to be trained on more data than Gemini and Gemini tried to avoid certain questions but ChatGPT answered all the questions it could.</p> <p>The following charts show the results mentioned above.</p> <p>Number of accurate responses in LLM responses. </p> <p>Percentage of hallucinations in LLM responses. </p>"},{"location":"#conclusion","title":"Conclusion","text":"<p>We were able to create a tool that can detect hallucinations in LLMs. Our tool successfully detects almost all the hallucinations in LLMs. However, our tool is not perfect and can still be improved since not all information is on the internet and some are inaccurate or contain biases.</p>"},{"location":"#future-works","title":"Future Works","text":"<p>Future works can use this methodology and combine with other techniques to improve the accuracy of hallucinations detection.</p>"},{"location":"#references","title":"References","text":"<ul> <li>A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation</li> <li>Automatic Calibration and Error Correction for Generative Large Language Models via Pareto Optimal Self-Supervision</li> <li>Check Your Facts and Try Again Improving Large Language Models with External Knowledge and Automated Feedback</li> <li>Constructing Benchmarks and Interventions for Combating Hallucinations in LLMs</li> <li>Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification</li> <li>HILL: A Hallucination Identifier for Large Language Models</li> <li>HalluVault: A Novel Logic Programming-aided Metamorphic Testing Framework for Detecting Fact-Conflicting Hallucinations in Large Language Models</li> <li>Hallucination Detection: Robustly Discerning Reliable Answers in Large Language Models</li> <li>Hallucination Reduction in Large Language Models with Retrieval-Augmented Generation Using Wikipedia Knowledge</li> <li>KnowHalu: Hallucination Detection via Multi-Form Knowledge Based Factual Checking</li> <li>Lynx: An Open Source Hallucination Evaluation Model</li> <li>Mitigating Entity-Level Hallucination in Large Language Models</li> <li>Mitigating Hallucinations in Large Language Models via Self-Refinement-Enhanced Knowledge Retrieval</li> <li>Reducing Hallucinations in Large Language Models A Consensus Voting Approach Using Mixture of Experts</li> <li>Reducing Hallucinations in Large Language Models Through Contextual Position Encoding</li> <li>Self-Contradictory Hallucinations of LLMs Evaluation, Detection and Mitigation</li> <li>The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models</li> <li>The Troubling Emergence of Hallucination in Large Language Models \u2013 An Extensive Definition, Quantification, and Prescriptive Remediations</li> <li>Truth-Aware Context Selection: Mitigating the Hallucinations of Large Language Models Being Misled by Untruthful Contexts</li> <li>Zero-Shot Multi-task Hallucination Detection</li> </ul>"},{"location":"api/","title":"ATLAS API","text":""},{"location":"api/#introduction","title":"Introduction","text":"<p>The ATLAS API is the bedrock if the entire project. Made up of three endpoints; <code>/api</code>, <code>/verify</code> and <code>/search</code>, it serves as the gateway to all operations from detection to mitigation.</p>"},{"location":"api/#enpoints","title":"Enpoints","text":""},{"location":"api/#api","title":"/api","text":"<p>The endpoint takes in a string query as parameter for a google search query as shown in the image below.</p> <p>Example</p> <p></p> <p>The <code>/api</code> endpoint returns the result of a google search query in json format as shown below.</p> <p>Example</p> <p></p> <p>Tip</p> <p>The <code>/api</code> endpoint utilizes the google search API to perform search queries on the web.</p>"},{"location":"api/#verify","title":"/verify","text":"<p>The enpoint takes in two parameters; the first one being the query given to the LLM. And the second being the response provided by the LLM.</p> <p>Example</p> <p></p> <p>The <code>/verify</code> endpoint returns an alert in the event that a hallucination has occurred or not.</p> <p>Example</p> <p></p> <p>Note</p> <p>It can be observed that in the figure above, the LLM provided the right response and hence did not raise an alert for hallucination detection.</p> <p>Tip</p> <p>The <code>/verify</code> endpoint utilizes google's gemini to scan the response provided by the LLM and compares it to the results we got from our google to detect or flag any occurrence of hallucinations.</p>"},{"location":"api/#search","title":"/search","text":"<p>This is the last endpoint in the suite of endpoints for the ATLAS project. This endpoint is however deprecated since it performs a loaded task of retrieving search results from the web using jina search and returning the results in markdown format.</p> <p>It takes in one parameter; the search query for the web as shown below.</p> <p>Example</p> <p></p> <p>It then returns the results from the web search rendered in markdown as shown below.</p> <p>Example</p> <p></p> <p>Tip</p> <p>Researchers can use the <code>/search</code> endpoint to scrape the web to train their LLMs in an effective way to aid in the fight to mitigate or eliminate hallucinations in LLMs.</p>"},{"location":"api/#license","title":"License","text":"<p>All the tools under the ATLAS suite are licensed under the MIT License.</p>"},{"location":"chrome/","title":"Chrome Extension","text":""},{"location":"chrome/#overview","title":"Overview","text":"<p>The Atlas Chrome Extension is a powerful tool designed to enhance your interaction with Large Language Models (LLMs) by providing real-time factual verification. This extension seamlessly integrates with popular AI platforms, offering a convenient side panel for detecting and mitigating potential hallucinations in LLM responses.</p>"},{"location":"chrome/#features","title":"Features","text":"<ul> <li>Automatic activation on supported AI platforms</li> <li>Easy-to-use side panel interface</li> <li>Web-based verification of LLM responses</li> <li>Keyboard shortcut for quick access</li> </ul>"},{"location":"chrome/#supported-platforms","title":"Supported Platforms","text":"<p>The extension activates automatically when you visit any of the following websites:</p> <ul> <li>ChatGPT</li> <li>Google Gemini</li> <li>Anthropic Claude</li> </ul> <p>Note</p> <p>Even though we have specified websites for it to work on, our extension would also work on any website as long as you can copy and paste the text.</p>"},{"location":"chrome/#installation","title":"Installation","text":"<ul> <li>Visit https://github.com/amarquaye/atlas-chrome and download the zip file as shown below.</li> </ul> <p>Example</p> <p></p> <ul> <li> <p>Extract the files into a folder of your choice.</p> </li> <li> <p>Open your browser(chromium based) and type <code>chrome://extensions</code> in my case i have to type <code>brave://extensions</code> in the address bar since i'm using brave; another chromium-based browser.</p> </li> <li> <p>Click on developer mode to toggle developer mode and unpack extension.</p> </li> <li> <p>Click on load unpacked as shown in the diagram below.</p> </li> </ul> <p>Example</p> <p></p> <ul> <li>Select the folder which you extracted the files into and click on Select Folder in the dialogue box.</li> </ul> <p>Example</p> <p></p> <ul> <li>Congrats, the installation is successful upon seeing this image as shown below.</li> </ul> <p>Example</p> <p></p>"},{"location":"chrome/#usage","title":"Usage","text":"<ol> <li>Activating the Side Panel</li> <li>Use the keyboard shortcut: <code>Ctrl + Shift + H</code></li> <li> <p>Or click the extension icon in the browser toolbar</p> </li> <li> <p>Verifying LLM Responses    a. Copy your original query to the LLM    b. Paste it into the \"LLM Query\" textarea in the side panel    c. Copy the LLM's response    d. Paste it into the \"LLM Response\" textarea    e. Click the \"Verify\" button</p> </li> <li> <p>Reviewing Results    The extension will search the web to verify the factuality of the LLM's response and present its findings directly in the side panel.</p> </li> </ol>"},{"location":"chrome/#user-interface","title":"User Interface","text":"<p>The side panel features a clean, intuitive interface with clearly labeled sections:</p> <ul> <li>LLM Query input area</li> <li>LLM Response input area</li> <li>Verify button</li> <li>Results display section</li> </ul> <p>This streamlined design ensures a smooth and efficient verification process, allowing users to quickly assess the accuracy of AI-generated content.</p> <p>Example</p> <p></p>"},{"location":"chrome/#benefits","title":"Benefits","text":"<ul> <li>Enhance the reliability of AI interactions</li> <li>Quickly identify potential misinformation or hallucinations</li> <li>Seamless integration with your existing AI workflow</li> <li>Improve decision-making based on AI-assisted information</li> </ul>"},{"location":"chrome/#technical-details","title":"Technical Details","text":"<p>To view and test the api, visit https://atlasproject-phi.vercel.app/docs to access the interactive documentation.</p>"},{"location":"chrome/#license","title":"License","text":"<p>All the tools under the ATLAS suite are licensed under the MIT License.</p>"}]}