# ATLAS üåç

Final year project by [Jesse Amarquaye](mailto:jesseamarquayelegendary@gmail.com "Send email") and Greatman Akomea, computer engineering students from [Ghana Communication Technology University](https://www.gctu.edu.gh "GCTU").

## Table Of Contents

- [Introduction](#introduction)
  - [Why work on hallucination in LLMs?](#why-work-on-hallucination-in-llms)
  - [Aims or Objectives](#aims-or-objectives)
- [Methodology](#methodology)

## Introduction

Atlas is a hallucination detector for Large Language Models.
Its main focus is on **generative text** as that is the most widely used medium for interacting with LLMs.

### Why work on hallucination in LLMs?

Large language models (LLMs) are revolutionizing human-computer interaction, generating increasingly _fluent_ and _human-like text_.
However, a significant challenge in LLMs is their tendency to produce **hallucinations**, or factually incorrect, nonsensical, or misleading content.
As humans become increasingly reliant on LLMs for information and decision-making, ensuring their reliability and accuracy is crucial.
This project aims to address this challenge by developing a software for **detecting** and **mitigating** hallucinations in LLMs so users can rely on LLM outputs with greater confidence, leading to wider adoption and societal benefits and also reduces the risk of misinformation and promotes responsible use of LLMs.

### Aims or Objectives

- [x] Explore techniques for mitigating hallucinations in LLMs.
- [ ] Develop a software for automatic detection of hallucinations in LLMs.
- [ ] Evaluate the effectiveness of the developed tool in different LLMs.

## Methodology

![Flow diagram](docs/img/flow.svg)
